---
title: 'Семинар 4. Факторный анализ'
date: 'Июнь, 7, 2018'
output:
  html_document:
    keep_md: no
    number_sections: yes
    toc: yes
lang: ru-RU
editor_options:
  chunk_output_type: console
---


Шаманское заклинание для настройки глобальных опций отчёта:
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```


```{r}
library(tidyverse) # обработка данных, графики...
library(skimr)# описательные статистики
library(rio) # импорт фантастического количества форматов данных

library(cluster) # кластерный анализ
library(factoextra) # визуализации kmeans, pca,
library(dendextend) # визуализация дендрограмм

library(corrplot) # визуализация корреляций

library(broom) # метла превращает результаты оценивания моделей в таблички

library(naniar) # визуализация пропущенных значений
library(visdat) # визуализация пропущенных значений

library(patchwork) # удобное расположение графиков рядом
```



# Немного формул для k-средних! :)


Снова возьмём данные по потреблению протеинов Европе из книги [Practial Machine Learning Cookbook](https://github.com/PacktPublishing/Practical-Machine-Learning-Cookbook/blob/master/Chapter%2003/Data/Europenaprotein.csv).
Загрузим их и посмотрим описательные статистики.
В наборе данных для каждой страны указано, сколько белка получает ежедневно житель из различных продуктов.

```{r}
protein <- import('Europenaprotein.csv')
skim(protein)
```

Отмасштабируем все числовые переменные с помощью функции `scale()`.
Затем спрячем текстовую переменную `Country` в названия строк:

```{r}
protein_no_country <- protein %>%
  mutate_if(is.numeric, ~ as.vector(scale(.))) %>%
  column_to_rownames(var = 'Country')
```

Дополнение в виде функции `as.vector` нужно потому, что функция `scale` возвращает матрицу,
а каждый столбец должен быть вектором :)


Выполним кластеризацию методом k-средних с помощью функции `kmeans`.

В качестве аргументов укажем отмасштабированные данные `protein_no_country` и количество кластеров `centers`.
Берём три кластера!
Сохраним результат кластеризации в список `k_means_protein`.

```{r}
k_means_protein <- kmeans(protein_no_country, centers = 3)
k_means_protein
```


Внутригрупповая сумма квадратов расстояний, WSS (within):
\[
WSS = ||x_1 - c_1||^2 + ||x_2 - c_2 ||^2 + \ldots + ||x_n - c_n||^2,
\]
где $x_i$ — координаты $i$-го наблюдения, $c_i$ — координаты центра кластера, к которому относится $i$-ое наблюдение.
У наблюдений из одного кластера величины $c_i$ равны.

Узнаем величину WSS с разбивкой по кластерам:
```{r}
k_means_protein$withinss
```

Межгрупповая сумма квадратов расстояний,  BSS (between):
\[
BSS = ||c_1 - \bar x||^2 + ||c_2 - \bar x||^2 + \ldots + ||c_n - \bar x||^2
\]
```{r}
k_means_protein$betweenss
```


Общая сумма квадратов расстояний, TSS (total):
\[
TSS = ||x_1 - \bar x||^2 + ||x_2 - \bar x||^2 + \ldots + ||x_n - \bar x||^2
\]
```{r}
k_means_protein$totss
```


Суть алгоритма $k$-средних в терминах квадратов расстояний проста:

Кластеры определяются так, чтобы величина WSS была минимально возможной.


По теореме Пифагора:
\[
TSS = WSS + BSS
\]


Как понять, сколько кластеров брать оптимально?
Один из способов сделать это — воспользоваться командой `fviz_nbclust` из пакета `factoextra`.


```{r}
g1 <- fviz_nbclust(protein_no_country, kmeans, method = 'wss') +
  labs(title = 'Зависимость WSS от числа кластеров',
       subtitle = 'Метод: на глаз!', x = 'Число кластеров',
       y = 'Внутригрупповая сумма квадратов расстояний')
g1
```


```{r}
g2 <- fviz_nbclust(protein_no_country, kmeans, method = 'silhouette') +
  labs(subtitle = 'Метод силуэтов',
       title = 'Зависимость средней ширины силуэта от числа кластеров',
       y = 'Средняя ширина силуэта по всем точкам',
       x = 'Число кластеров')
g2
```

Обозначения:

* $a_i$ — среднее расстояние от точки $i$ до точек своего кластера;
* $b_i$ — среднее расстояние от точки $i$ до точек ближайшего кластера-конкурента;

* Ширина силуэта точки $i$:

\[
s_i = \frac{b_i - a_i}{\max\{a_i, b_i\}}
\]


```{r}
g3 <- fviz_nbclust(protein_no_country, kmeans, method = 'gap_stat') +
  labs(subtitle = 'Gap statistic method')
g3
```

Обозначения:

* $k$ — число кластеров
* $WSS_{random}$ — средняя величина внутригрупповой суммы квадратов расстояний,
если генерировать точки равномерно в прямоугольнике имеющихся данных.


Статистика разрыва (gap-статистика):
\[
Gap(k) = \ln WSS_{random}(k) - \ln WSS(k)
\]

На графике показаны значения статистики разрыва плюс
доверительный интервал шириной в одной стандартное отклонение.


Автоматически компьютер подбирает такое количество количество кластеров $k$,
начиная с которого статистика разрыва растёт слишком медленно или вовсе падает.
«Слишком медленно» растёт означает, что следующее значение статистики попадает в доверительный интервал предыдущего.

Более формально, автоматически находится первое $k$ при котором:
\[
Gap(k) + 1 \cdot se(Gap(k)) \geq Gap(k+1)
\]



# Иерархическая кластеризация

Другой способ разбить данные на группы — иерархическая кластеризация.
Но, в отличие от метода k-средних, она работает с матрицей расстояний,
поэтому первым делом посчитаем её!
Для этого будем использовать функцию `dist()`.
Передадим ей стандартизированные данные и укажем явно, как считать расстояния с помощью аргумента `method`.
О всех остальных опциях можно узнать в справке.

```{r}
protein_dist <- dist(protein_no_country, method = 'euclidian')
```

Расстояния тоже можно визуализировать!
Сделаем это командой `fviz_dist` из пакета `factoextra`.

```{r}
fviz_dist(protein_dist)
```




* Упражнение 1.


Посчитайте матрицу расстояний для таблицы `usa_stand` и визуализируйте её.

```{r}
# usa <- USArrests
# usa_dist <- dist(___, method = 'euclidian')
# fviz_dist(___)
```

Полученную матрицу расстояний можно передадать функции `hclust()`, которая кластеризует данные.
Однако в пакете `factoextra` есть функция `hcut()`, которая работает с исходными данными.
Будем использовать её и попросим выделить четыре кластера в аргументе `k`.


```{r}
protein_hcl <- hcut(protein_no_country, k = 4,
                    hc_metric = 'euclidean', hc_method = 'ward.D2')
```

Возможны бесчиленные вариации алгоритма!

По методу подсчёта расстояния между двумя точками, `hc_metric = `

* 'euclidean' — привычное Евклидово расстояние;
* 'manhattan' — расстояние по Майкопски! :)

![Майкоп](https://upload.wikimedia.org/wikipedia/commons/thumb/0/0a/Maikopmap.gif/1200px-Maikopmap.gif)

По методу подсчёта расстояний между кластерами, `hc_method =`

* 'single' — расстояние между ближайшими точками этих кластеров;
* 'complete' — расстояние между самыми далёкими точками этих кластеров;
* 'average' — среднее расстояние между точками кластеров;
* 'ward.D2' — на сколько увеличится WSS, если слить два кластера в один;



С помощью функции `fviz_dend` визуализируем результат кластеризации.
Укажем несколько аргументов, чтобы сделать дендрограмму красивее,
а полный перечень найдётся в справке.

```{r}
fviz_dend(protein_hcl,
          cex = 0.5, # размер подписи
          color_labels_by_k = TRUE) # цвет подписей по группам
```

Выявленные кластеры можно добавить к исходным данным!
```{r}
protein_plus2 <- mutate(protein, cluster = protein_hcl$cluster)
glimpse(protein_plus2)
```


* Упражнение 2.

- Сделайте иерархическую кластеризацию с четыремя группами на данных об арестах.

- Визуализируйте результат кластеризации и сделайте подписи цветными.

```{r}
# usa_hcl <- hcut(___, k = ___)
# fviz_dend(___,
#          cex = 0.5, # размер подписи
#          color_labels_by_k = ___) # цвет подписей по группам
```

Иерархичская кластеризация полезна и для визуализаций корреляций.
Если в функции `corrplot()` из одноимённого пакета указать аргумента `order = hclust`,
то мы получим сгруппированные по кластерам переменные.
Для красоты добавим ещё один аргумент — `addrect = 3`.
Он обведёт прямоугольниками указанное число кластеров.

```{r}
protein_cor <- cor(protein_no_country)
corrplot(protein_cor, order = 'hclust', addrect = 3)
```

* Упражнение 3.

Визуализируйте корреляции в данных об арестах `usa` и сгруппируйте их по двум кластерам.
Замените кружочки на квадраты, передав аргументу `method` значение `shade`.

```{r}
# usa_cor <- cor(___)
# corrplot(___, order = ___, addrect = ___, ___ = ___)
```

* Упражнение 4.

Добавьте к исходным данным `usa` кластеры, полученные с помощью иерархической кластеризации:
```{r}
# usa_plus2 <- mutate(___, cluster = ___)
# glimpse(___)
```

Визуализации [кластеров в известных наборах данных] (https://cran.r-project.org/web/packages/dendextend/vignettes/Cluster_Analysis.html)


# Метод главных компонент

Метод главных компонент: заменяем большое количество исходных переменных на меньшее количество
новых искусственных переменных, главных компонент.

Например, можно заменить 20 исходных переменных на две искусственные главные компоненты,
чтобы изобразить многомерный набор данных на двумерном графике.

Как переменные превращают в главные компоненты?

Для удобства представим, что у нас есть три исходные переменные, они центрированы и приведены к общему масштабу.

Подход А. Максимизация разброса.

В первую компоненту берём исходные три переменные с такими весами, чтобы:

- максимизировать разброс первой главной компоненты;
- сумма квадратов весов равнялась бы единице.

Формально:
\[
pc^{1}_i = \alpha_1 x_i + \alpha_2 y_i + \alpha_3 z_i
\]

МГК максимизирует $TSS(pc^1)=\sum (pc^1_i - 0)^2$ при ограничении $\alpha_1^2 + \alpha_2^2 + \alpha_3^2=1$.

Подход Б. Минимизация расстояний от точек до новой системы координат.

Мы хотим ввести новую прямую координат так,
чтобы прямая проходила на минимальном расстоянии от имеющихся точек.

\[
\sum (x_i - \hat x_i)^2 + \sum (y_i - \hat y_i)^2 + \sum (z_i - \hat z_i)^2 \to \min,
\]
при условии, что точки $\hat x_i$, $\hat y_i$, $\hat z_i$ лежат на одной прямой.

Эти подходы эквивалентны.

После применения метода главных компонент оказывается, что:

\[
TSS(pc^1) + TSS(pc^2) + TSS(pc^3) = TSS(x) + TSS(y) + TSS(z),
\]
где $TSS$ — сумма квадратов значений переменной. Напомним, что переменные у нас центрированы.


Реализуем метод главных компонент на данных о потреблении белка функцией `prcomp()`.
Передадим ей отмасштабированные данные `protein_no_country`, хотя так поступать и необязательно.
Другой путь — передать исходный набора данных и попросить функцию `procmp()` стандартизировать их
аргументом  `scale = TRUE`.

```{r}
protein_pca <- prcomp(protein_no_country)
protein_pca
```

Посмотрим, что лежит в этом списке!

```{r}
attributes(protein_pca)
```

Сами новые искусственные главные компоненты лежат в матрице `protein_pca$x`.
Например, первая главная компонента:
```{r}
protein_pca$x[, 1]
```

Выборочные стандартные отклонения компонент (корни из $\lambda_j$) лежат в векторе `protein_pca$sdev`:
```{r}
protein_pca$sdev
```

Матрица `protein_pca$rotation` содержит веса, с которыми исходные переменные входят в новые искуственные главные компоненты. Например, в первой главной компоненте лежат исходные переменные с весами:
```{r}
protein_pca$rotation[, 1]
```


Визуализируем данные в осях первых двух главных компонент.
Для этого воспользуемся функцией `fviz_pca_ind()` из пакета `factoextra`.
Рисовать будем `protein_pca`, а аргумент `repel = TRUE` укажем для того,
чтобы подписи на графике не перекрывали друг друга.

```{r}
fviz_pca_ind(protein_pca, repel = TRUE)
```

* Упражнение 5.

- Выделите главные компоненты на данных об арестах в Америке.
Будьте внимательны: эти данные мы не масштабировали!

- Визуализируйте данные в осях первых двух главных компонент.
Проследите, чтобы подписи точек на графике были аккуратными :)

```{r}
# usa_pca <- prcomp(___, scale. = ___)
# usa_pca
# fviz_pca_ind(___, repel = ___)
```

- Какой процент дисперсии объясняют в сумме первые две главные компоненты?
- Какой из штатов скорее можно назвать нетипичным, Калифорнию или Вирджинию?
- Какой из штатов наиболее похож на Техас?

Помимо самих данных, в осях главных компонент можно нарисовать проекции исходных переменных.
Сделаем это командой `fviz_pca_biplot()`.
Как и прежде, укажем, что мы хотим изобразить, и попросим сделать подписи аккуратными
аргументом `repel = TRUE`.

```{r}
fviz_pca_biplot(protein_pca, repel = TRUE)
```

- Какая исходная переменная входит в первую главную компоненту с большим положительным весом?
- Какая исходная переменная входит в первую главную компоненту с большим отрицательным весом?
- Как можно хотя бы примерно проинтепретировать первую главную компоненту?


* Упражнение 6.

Визуализируйте в осях первых двух главных компонент американские штаты и проекции
исходных переменных.

```{r}
# fviz_pca_biplot(___, ___ = TRUE)
```

- Какая исходная переменная входит в первую главную компоненту с большим отрицательным весом?
- Как можно хотя бы примерно проинтепретировать первую главную компоненту?
- Какие штаты можно считать самыми безопасными?


Теперь изобразим, какой процент разброса данных объясняет каждая главная компонента.
Будем использовать команду `fviz_eig()` из пакета `factoextra`.

```{r}
fviz_eig(protein_pca)
```

- Сколько нужно взять главных компонент, чтобы объяснить более 70% разброса исходных наблюдений?

* Упражнение 7.

Визиулизируйте процент разброса, который объясняет каждая главная компонента.
Обратите внимание, что у функции появился новый аргумент :)

```{r}
# fviz_eig(___, addlabels = TRUE)
```

- Какой процент разброса в данных объясняют в сумме третья и четвёртая глаыне компоненты?
- Что меняет аргумент `addlabels = TRUE`?


Выясним, какой влкад вносит каждая переменная в конкретную главную компоненту.
Для этого будем использовать команду `fviz_contrib()`.
Передадим ей объект `protein_pca` и в качестве аргументов укажем `choice = 'var'`,
так как нам нужны именно переменные, а не наблюдения, и `axes = 1`,
чтобы посмотреть на первую главную компоненту.
Для визуализации вклада наблюдения значение аргумента `choice` поменяем на `ind`
и для разнообразия посмотрим на третью главную компоненту :)

```{r}
fviz_contrib(protein_pca, choice = 'var', axes = 1)
fviz_contrib(protein_pca, choice = 'ind', axes = 3)
```

- Какие переменные имеют наибольший вес в первой главной компоненте?
- Какая страна вносит наибольший вклад в сумму квадратов расстояний до центра для третьей главной компоненты?

* Упражнение 8.

Для данных по арестам визуализируйте вклад переменных во вторую главную компоненту
и вклад наблюдений в первую.

```{r}
# fviz_contrib(___, choice = '___', axes = ___)
# fviz_contrib(___, choice = ___, ___ = ___)
```


В осях первых двух главных компонент исходные данные можно раскрасить согласно кластерам:

```{r}
fviz_cluster(object = k_means_protein,
             data = protein_no_country,
             ellipse.type = 'convex')
```

Теперь мы понимаем, что на этом графике!

Ура! :)
